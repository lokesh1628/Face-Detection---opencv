{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "lr = 1e-3\n",
    "batch_size = 64\n",
    "img_dims = (96,96,3)\n",
    "\n",
    "data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = [f for f in glob.glob(r'D://Gender-Detection-master//gender_dataset_face' + \"/**/*\", recursive=True) if not os.path.isdir(f)]\n",
    "random.shuffle(image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting images to arrays and labelling the categories\n",
    "for img in image_files:\n",
    "\n",
    "    image = cv2.imread(img)\n",
    "    \n",
    "    image = cv2.resize(image, (img_dims[0],img_dims[1]))\n",
    "    image = img_to_array(image)\n",
    "    data.append(image)\n",
    "\n",
    "    label = img.split(os.path.sep)[-2] # C:\\Files\\gender_dataset_face\\woman\\face_1162.jpg\n",
    "    if label == \"woman\":\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "        \n",
    "    labels.append([label]) # [[1], [0], [0], ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.2,\n",
    "                                                  random_state=42)\n",
    "\n",
    "trainY = to_categorical(trainY, num_classes=2) # [[1, 0], [0, 1], [0, 1], ...]\n",
    "testY = to_categorical(testY, num_classes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmenting datset \n",
    "aug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n",
    "                         height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
    "                         horizontal_flip=True, fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "def build(width, height, depth, classes):\n",
    "    model = Sequential()\n",
    "    inputShape = (height, width, depth)\n",
    "    chanDim = -1\n",
    "\n",
    "    if K.image_data_format() == \"channels_first\": #Returns a string, either 'channels_first' or 'channels_last'\n",
    "        inputShape = (depth, height, width)\n",
    "        chanDim = 1    \n",
    "    \n",
    "    # The axis that should be normalized, after a Conv2D layer with data_format=\"channels_first\", \n",
    "    # set axis=1 in BatchNormalization.\n",
    "\n",
    "    model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=inputShape))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(128, (3,3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "\n",
    "    model.add(Conv2D(128, (3,3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(classes))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "model = build(width=img_dims[0], height=img_dims[1], depth=img_dims[2],\n",
    "                            classes=2)\n",
    "\n",
    "# compile the model\n",
    "opt = Adam(lr=lr, decay=lr/epochs)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-21-2967944a434a>:6: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "29/28 [==============================] - 36s 1s/step - loss: 0.8121 - accuracy: 0.6943\n",
      "Epoch 2/100\n",
      "29/28 [==============================] - 37s 1s/step - loss: 0.5652 - accuracy: 0.7783\n",
      "Epoch 3/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.5018 - accuracy: 0.7978\n",
      "Epoch 4/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.4385 - accuracy: 0.8396\n",
      "Epoch 5/100\n",
      "29/28 [==============================] - 41s 1s/step - loss: 0.4100 - accuracy: 0.8385\n",
      "Epoch 6/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.3568 - accuracy: 0.8596\n",
      "Epoch 7/100\n",
      "29/28 [==============================] - 41s 1s/step - loss: 0.3299 - accuracy: 0.8770\n",
      "Epoch 8/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.3331 - accuracy: 0.8705\n",
      "Epoch 9/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.2990 - accuracy: 0.8867\n",
      "Epoch 10/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.2387 - accuracy: 0.9138\n",
      "Epoch 11/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.2460 - accuracy: 0.9057\n",
      "Epoch 12/100\n",
      "29/28 [==============================] - 39s 1s/step - loss: 0.2241 - accuracy: 0.9068\n",
      "Epoch 13/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.2076 - accuracy: 0.9160\n",
      "Epoch 14/100\n",
      "29/28 [==============================] - 40s 1s/step - loss: 0.1956 - accuracy: 0.9220\n",
      "Epoch 15/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.1651 - accuracy: 0.9339\n",
      "Epoch 16/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.1626 - accuracy: 0.9398\n",
      "Epoch 17/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.1494 - accuracy: 0.9474\n",
      "Epoch 18/100\n",
      "29/28 [==============================] - 39s 1s/step - loss: 0.1387 - accuracy: 0.9512\n",
      "Epoch 19/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.1254 - accuracy: 0.9523\n",
      "Epoch 20/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.1138 - accuracy: 0.9556\n",
      "Epoch 21/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.1171 - accuracy: 0.9588\n",
      "Epoch 22/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.1377 - accuracy: 0.9550\n",
      "Epoch 23/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.1135 - accuracy: 0.9561\n",
      "Epoch 24/100\n",
      "29/28 [==============================] - 39s 1s/step - loss: 0.1156 - accuracy: 0.9528\n",
      "Epoch 25/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.1207 - accuracy: 0.9561\n",
      "Epoch 26/100\n",
      "29/28 [==============================] - 37s 1s/step - loss: 0.0924 - accuracy: 0.9659\n",
      "Epoch 27/100\n",
      "29/28 [==============================] - 37s 1s/step - loss: 0.1124 - accuracy: 0.9588\n",
      "Epoch 28/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.1106 - accuracy: 0.9572\n",
      "Epoch 29/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.1104 - accuracy: 0.9588\n",
      "Epoch 30/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.1170 - accuracy: 0.9496\n",
      "Epoch 31/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.0996 - accuracy: 0.9615\n",
      "Epoch 32/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.1046 - accuracy: 0.9610\n",
      "Epoch 33/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.0957 - accuracy: 0.9604\n",
      "Epoch 34/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.1001 - accuracy: 0.9664\n",
      "Epoch 35/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.0903 - accuracy: 0.9642\n",
      "Epoch 36/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.0998 - accuracy: 0.9604\n",
      "Epoch 37/100\n",
      "29/28 [==============================] - 39s 1s/step - loss: 0.0819 - accuracy: 0.9729\n",
      "Epoch 38/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.0822 - accuracy: 0.9696\n",
      "Epoch 39/100\n",
      "29/28 [==============================] - 39s 1s/step - loss: 0.0887 - accuracy: 0.9686\n",
      "Epoch 40/100\n",
      "29/28 [==============================] - 39s 1s/step - loss: 0.0856 - accuracy: 0.9680\n",
      "Epoch 41/100\n",
      "29/28 [==============================] - 42s 1s/step - loss: 0.0736 - accuracy: 0.9724\n",
      "Epoch 42/100\n",
      "29/28 [==============================] - 41s 1s/step - loss: 0.0788 - accuracy: 0.9691\n",
      "Epoch 43/100\n",
      "29/28 [==============================] - 40s 1s/step - loss: 0.0944 - accuracy: 0.9637\n",
      "Epoch 44/100\n",
      "29/28 [==============================] - 45s 2s/step - loss: 0.0743 - accuracy: 0.9740\n",
      "Epoch 45/100\n",
      "29/28 [==============================] - 40s 1s/step - loss: 0.0704 - accuracy: 0.9740\n",
      "Epoch 46/100\n",
      "29/28 [==============================] - 39s 1s/step - loss: 0.0766 - accuracy: 0.9702\n",
      "Epoch 47/100\n",
      "29/28 [==============================] - 39s 1s/step - loss: 0.0677 - accuracy: 0.9734\n",
      "Epoch 48/100\n",
      "29/28 [==============================] - 39s 1s/step - loss: 0.0611 - accuracy: 0.9756\n",
      "Epoch 49/100\n",
      "29/28 [==============================] - 39s 1s/step - loss: 0.0631 - accuracy: 0.9756\n",
      "Epoch 50/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.0479 - accuracy: 0.9821\n",
      "Epoch 51/100\n",
      "29/28 [==============================] - 39s 1s/step - loss: 0.0629 - accuracy: 0.9794\n",
      "Epoch 52/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.0619 - accuracy: 0.9756\n",
      "Epoch 53/100\n",
      "29/28 [==============================] - 39s 1s/step - loss: 0.0704 - accuracy: 0.9729\n",
      "Epoch 54/100\n",
      "29/28 [==============================] - 40s 1s/step - loss: 0.0741 - accuracy: 0.9702\n",
      "Epoch 55/100\n",
      "29/28 [==============================] - 39s 1s/step - loss: 0.0720 - accuracy: 0.9718\n",
      "Epoch 56/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.0636 - accuracy: 0.9751\n",
      "Epoch 57/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.0723 - accuracy: 0.9745\n",
      "Epoch 58/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.0682 - accuracy: 0.9718\n",
      "Epoch 59/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.0702 - accuracy: 0.9734\n",
      "Epoch 60/100\n",
      "29/28 [==============================] - 39s 1s/step - loss: 0.0430 - accuracy: 0.9864\n",
      "Epoch 61/100\n",
      "29/28 [==============================] - 39s 1s/step - loss: 0.0481 - accuracy: 0.9843\n",
      "Epoch 62/100\n",
      "29/28 [==============================] - 39s 1s/step - loss: 0.0515 - accuracy: 0.9772\n",
      "Epoch 63/100\n",
      "29/28 [==============================] - 39s 1s/step - loss: 0.0558 - accuracy: 0.9805\n",
      "Epoch 64/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.0677 - accuracy: 0.9767\n",
      "Epoch 65/100\n",
      "29/28 [==============================] - 41s 1s/step - loss: 0.0715 - accuracy: 0.9724\n",
      "Epoch 66/100\n",
      "29/28 [==============================] - 41s 1s/step - loss: 0.0597 - accuracy: 0.9778\n",
      "Epoch 67/100\n",
      "29/28 [==============================] - 39s 1s/step - loss: 0.0599 - accuracy: 0.9756\n",
      "Epoch 68/100\n",
      "29/28 [==============================] - 40s 1s/step - loss: 0.0569 - accuracy: 0.9789\n",
      "Epoch 69/100\n",
      "29/28 [==============================] - 40s 1s/step - loss: 0.0573 - accuracy: 0.9767\n",
      "Epoch 70/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.0647 - accuracy: 0.9745\n",
      "Epoch 71/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.0549 - accuracy: 0.9783\n",
      "Epoch 72/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.0630 - accuracy: 0.9734\n",
      "Epoch 73/100\n",
      "29/28 [==============================] - 39s 1s/step - loss: 0.0527 - accuracy: 0.9827\n",
      "Epoch 74/100\n",
      "29/28 [==============================] - 39s 1s/step - loss: 0.0539 - accuracy: 0.9810\n",
      "Epoch 75/100\n",
      "29/28 [==============================] - 39s 1s/step - loss: 0.0599 - accuracy: 0.9767\n",
      "Epoch 76/100\n",
      "29/28 [==============================] - 39s 1s/step - loss: 0.0506 - accuracy: 0.9821\n",
      "Epoch 77/100\n",
      "29/28 [==============================] - 39s 1s/step - loss: 0.0500 - accuracy: 0.9810\n",
      "Epoch 78/100\n",
      "29/28 [==============================] - 39s 1s/step - loss: 0.0392 - accuracy: 0.9843\n",
      "Epoch 79/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.0446 - accuracy: 0.9848\n",
      "Epoch 80/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.0619 - accuracy: 0.9762\n",
      "Epoch 81/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.0559 - accuracy: 0.9805\n",
      "Epoch 82/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.0545 - accuracy: 0.9767\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/28 [==============================] - 38s 1s/step - loss: 0.0471 - accuracy: 0.9859\n",
      "Epoch 84/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.0437 - accuracy: 0.9832\n",
      "Epoch 85/100\n",
      "29/28 [==============================] - 40s 1s/step - loss: 0.0427 - accuracy: 0.9843\n",
      "Epoch 86/100\n",
      "29/28 [==============================] - 41s 1s/step - loss: 0.0299 - accuracy: 0.9886\n",
      "Epoch 87/100\n",
      "29/28 [==============================] - 43s 1s/step - loss: 0.0248 - accuracy: 0.9924\n",
      "Epoch 88/100\n",
      "29/28 [==============================] - 39s 1s/step - loss: 0.0350 - accuracy: 0.9875\n",
      "Epoch 89/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.0320 - accuracy: 0.9886\n",
      "Epoch 90/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.0283 - accuracy: 0.9892\n",
      "Epoch 91/100\n",
      "29/28 [==============================] - 39s 1s/step - loss: 0.0281 - accuracy: 0.9902\n",
      "Epoch 92/100\n",
      "29/28 [==============================] - 39s 1s/step - loss: 0.0293 - accuracy: 0.9908\n",
      "Epoch 93/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.0324 - accuracy: 0.9870\n",
      "Epoch 94/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.0537 - accuracy: 0.9821\n",
      "Epoch 95/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.0360 - accuracy: 0.9859\n",
      "Epoch 96/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.0364 - accuracy: 0.9848\n",
      "Epoch 97/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.0472 - accuracy: 0.9821\n",
      "Epoch 98/100\n",
      "29/28 [==============================] - 38s 1s/step - loss: 0.0471 - accuracy: 0.9827\n",
      "Epoch 99/100\n",
      "29/28 [==============================] - 41s 1s/step - loss: 0.0470 - accuracy: 0.9837\n",
      "Epoch 100/100\n",
      "29/28 [==============================] - 39s 1s/step - loss: 0.0369 - accuracy: 0.9886\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "#H=model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
    " #         steps_per_epoch=len(x_train) / 32, epochs=epochs)\n",
    "\n",
    "\n",
    "H = model.fit_generator(datagen.flow(trainX, trainY, batch_size=64),\n",
    "                        validation_data=(testX,testY),\n",
    "                        steps_per_epoch=len(trainX) / 64,\n",
    "                        epochs=epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gender_detection.model\\assets\n"
     ]
    }
   ],
   "source": [
    "# save the model to disk\n",
    "model.save('gender_detection.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-da43151ce597>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"train_loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"val_loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"val_loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"acc\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"train_acc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"val_acc\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"val_acc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training/validation loss/accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "N = epochs\n",
    "plt.plot(np.arange(0,N), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0,N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0,N), H.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0,N), H.history[\"val_acc\"], label=\"val_acc\")\n",
    "\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"upper right\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save plot to disk\n",
    "plt.savefig('plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEBCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import cvlib as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = load_model('gender_detection.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open webcam\n",
    "webcam = cv2.VideoCapture(0)\n",
    "\n",
    "classes = ['man','woman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e72d0c78f473>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# apply face detection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mface\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfidence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect_face\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# loop through frames\n",
    "while webcam.isOpened():\n",
    "\n",
    "    # read frame from webcam \n",
    "    status, frame = webcam.read()\n",
    "\n",
    "    # apply face detection\n",
    "    face, confidence = cv.detect_face(frame)\n",
    "\n",
    "\n",
    "    # loop through detected faces\n",
    "    for idx, f in enumerate(face):\n",
    "\n",
    "        # get corner points of face rectangle        \n",
    "        (startX, startY) = f[0], f[1]\n",
    "        (endX, endY) = f[2], f[3]\n",
    "\n",
    "        # draw rectangle over face\n",
    "        cv2.rectangle(frame, (startX,startY), (endX,endY), (0,255,0), 2)\n",
    "\n",
    "        # crop the detected face region\n",
    "        face_crop = np.copy(frame[startY:endY,startX:endX])\n",
    "\n",
    "        if (face_crop.shape[0]) < 10 or (face_crop.shape[1]) < 10:\n",
    "            continue\n",
    "\n",
    "        # preprocessing for gender detection model\n",
    "        face_crop = cv2.resize(face_crop, (96,96))\n",
    "        face_crop = face_crop.astype(\"float\") / 255.0\n",
    "        face_crop = img_to_array(face_crop)\n",
    "        face_crop = np.expand_dims(face_crop, axis=0)\n",
    "\n",
    "        # apply gender detection on face\n",
    "        conf = model.predict(face_crop)[0] # model.predict return a 2D matrix, ex: [[9.9993384e-01 7.4850512e-05]]\n",
    "\n",
    "        # get label with max accuracy\n",
    "        idx = np.argmax(conf)\n",
    "        label = classes[idx]\n",
    "\n",
    "        label = \"{}: {:.2f}%\".format(label, conf[idx] * 100)\n",
    "\n",
    "        Y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "\n",
    "        # write label and confidence above face rectangle\n",
    "        cv2.putText(frame, label, (startX, Y),  cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.7, (0, 255, 0), 2)\n",
    "\n",
    "    # display output\n",
    "    cv2.imshow(\"gender detection\", frame)\n",
    "\n",
    "    # press \"Q\" to stop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# release resources\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (1.18.5)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (4.5.1.48)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (2.24.0)\n",
      "Collecting progressbar\n",
      "  Downloading progressbar-2.5.tar.gz (10 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests) (2020.6.20)\n",
      "Building wheels for collected packages: progressbar\n",
      "  Building wheel for progressbar (setup.py): started\n",
      "  Building wheel for progressbar (setup.py): finished with status 'done'\n",
      "  Created wheel for progressbar: filename=progressbar-2.5-py3-none-any.whl size=12078 sha256=3caa79ff77382be0958608b8f4e07e47685a4bf27d2772517a190dcec8756e5d\n",
      "  Stored in directory: c:\\users\\lenovo\\appdata\\local\\pip\\cache\\wheels\\2c\\67\\ed\\d84123843c937d7e7f5ba88a270d11036473144143355e2747\n",
      "Successfully built progressbar\n",
      "Installing collected packages: progressbar\n",
      "Successfully installed progressbar-2.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\LENOVO\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy opencv-python requests progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
